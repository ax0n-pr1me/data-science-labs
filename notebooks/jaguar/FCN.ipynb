{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a Python code example using a Fully Convolutional Network (FCN) for semantic segmentation with 4x4 pixel chips. This example uses PyTorch for the FCN implementation and handles input images that are not perfectly divisible by the chip size by padding the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Function to extract pixel chips\n",
    "\n",
    "\n",
    "def extract_pixel_chips(gdf, raster, chip_size=4):\n",
    "    half_chip = chip_size // 2\n",
    "    values = []\n",
    "    for point in gdf.geometry:\n",
    "        row, col = raster.index(point.x, point.y)\n",
    "        pixel_values = raster.read(\n",
    "        )[:, row-half_chip:row+half_chip, col-half_chip:col+half_chip]\n",
    "        values.append(pixel_values)\n",
    "    return np.array(values)\n",
    "\n",
    "# Fully Convolutional Network (FCN) for semantic segmentation\n",
    "\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(FCN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "# Function to pad the input image so it's dimensions are divisible by the chip size\n",
    "\n",
    "\n",
    "def pad_image(image, chip_size=4):\n",
    "    rows, cols = image.shape[1], image.shape[2]\n",
    "    pad_rows = chip_size - (rows % chip_size) if rows % chip_size != 0 else 0\n",
    "    pad_cols = chip_size - (cols % chip_size) if cols % chip_size != 0 else 0\n",
    "    padded_image = np.pad(image, ((0, 0), (0, pad_rows),\n",
    "                          (0, pad_cols)), mode='constant')\n",
    "    return padded_image\n",
    "\n",
    "\n",
    "# Read CSV and create GeoDataFrame\n",
    "csv_file = 'jaguar_locations.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "\n",
    "# Read satellite image\n",
    "satellite_image = 'satellite_image.tif'\n",
    "with rasterio.open(satellite_image) as src:\n",
    "    image = src.read()\n",
    "    profile = src.profile\n",
    "\n",
    "# Pad the image if its dimensions are not divisible by the chip size\n",
    "image = pad_image(image, chip_size=4)\n",
    "\n",
    "# Extract pixel chips and labels\n",
    "X = extract_pixel_chips(gdf, src)\n",
    "y = gdf['habitat'].values\n",
    "\n",
    "# Train the FCN\n",
    "# Note: For a complete example, you would need to split the data into training and testing sets,\n",
    "# create a PyTorch dataset and dataloader, and train the FCN using the dataloader.\n",
    "# This is just an outline of the process.\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "input_channels = X.shape[1]\n",
    "model = FCN(input_channels, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# After training the model, perform semantic segmentation on the input image by applying\n",
    "# the model on the entire padded image, and then crop the result back to the original image dimensions\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_image = torch.tensor(image[np.newaxis], dtype=torch.float32)\n",
    "    output = model(input_image)\n",
    "    predictions = output.argmax(dim=1).numpy()[0]\n",
    "\n",
    "# Crop the padded predictions back to the original image dimensions\n",
    "original_rows, original_cols = profile[\"height\"], profile[\"width\"]\n",
    "predictions_cropped = predictions[:original_rows, :original_cols]\n",
    "\n",
    "# Save the classified image as a new GeoTIFF\n",
    "output_file = 'classified_jaguar_habitat.tif'\n",
    "\n",
    "output_profile = profile.copy()\n",
    "output_profile.update(\n",
    "    dtype=rasterio.uint8,\n",
    "    count=1,\n",
    "    compress='lzw'\n",
    ")\n",
    "\n",
    "with rasterio.open(output_file, 'w', **output_profile) as dst:\n",
    "    dst.write(predictions_cropped.astype(rasterio.uint8), 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code shows how to perform semantic segmentation on the entire (padded) input image using the trained FCN model. The resulting classified image is then cropped back to the original image dimensions and saved as a new GeoTIFF file. Note that the training step is not included in this code snippet; you would need to create a PyTorch dataset and dataloader, and train the FCN using the dataloader as mentioned in the comments of the previous code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for training the FCN model using PyTorch:\n",
    "\n",
    "# Custom Dataset class\n",
    "class JaguarHabitatDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders for training and testing\n",
    "train_dataset = JaguarHabitatDataset(X_train, y_train)\n",
    "test_dataset = JaguarHabitatDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training the FCN model\n",
    "epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluate the trained model on the test dataset\n",
    "model.eval()\n",
    "test_accuracy = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        correct = predictions.eq(labels).sum().item()\n",
    "        test_accuracy += correct\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy /= total_samples\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
